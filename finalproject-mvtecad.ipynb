{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2454606,"sourceType":"datasetVersion","datasetId":1485601},{"sourceId":10050667,"sourceType":"datasetVersion","datasetId":6192573},{"sourceId":10090009,"sourceType":"datasetVersion","datasetId":6221663},{"sourceId":203994,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":174038,"modelId":196386}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torchsummary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:30:36.663278Z","iopub.execute_input":"2024-12-19T18:30:36.663615Z","iopub.status.idle":"2024-12-19T18:30:41.034942Z","shell.execute_reply.started":"2024-12-19T18:30:36.663587Z","shell.execute_reply":"2024-12-19T18:30:41.034241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport os, shutil\nimport matplotlib.pyplot as plt \n\nfrom PIL import Image\n\n\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\nfrom torchsummary import summary\nfrom torch.utils.data.dataset import Subset\nfrom torch import nn\nimport torch.optim as optim\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay, f1_score\nimport seaborn as sns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:30:41.036687Z","iopub.execute_input":"2024-12-19T18:30:41.036989Z","iopub.status.idle":"2024-12-19T18:31:08.847179Z","shell.execute_reply.started":"2024-12-19T18:30:41.036958Z","shell.execute_reply":"2024-12-19T18:31:08.846415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize some samples","metadata":{}},{"cell_type":"code","source":"image_path = '/kaggle/input/goodsad/cigarette_box/cigarette_box/test/opened/000_004.jpg'\nimage = Image.open(image_path)\nprint(image.size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:31.301739Z","iopub.execute_input":"2024-12-18T16:54:31.302087Z","iopub.status.idle":"2024-12-18T16:54:31.352495Z","shell.execute_reply.started":"2024-12-18T16:54:31.302049Z","shell.execute_reply":"2024-12-18T16:54:31.351679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the transformation pipeline using torchvision.transforms.Compose\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),  # Resize the image to 224x224 pixels\n    transforms.ToTensor()          # Convert the image to a PyTorch tensor and divide by 255.0\n])\n\n# Assuming 'image' is a PIL image object\n# Apply the defined transformation pipeline to the image\nimage = transform(image)\n\n# Print the shape of the transformed image tensor\nprint(image.shape)\n\n# Plot the transformed image\n# Permute the dimensions to (height, width, channels) as matplotlib expects\nplt.imshow(image.permute(1, 2, 0))\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:31.353425Z","iopub.execute_input":"2024-12-18T16:54:31.353673Z","iopub.status.idle":"2024-12-18T16:54:31.815347Z","shell.execute_reply.started":"2024-12-18T16:54:31.353649Z","shell.execute_reply":"2024-12-18T16:54:31.814517Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Memory consumed by the tensor","metadata":{}},{"cell_type":"code","source":"# Calculate the memory usage\nmemory_usage = image.numel() * image.element_size()\n\n# Print the memory usage\nprint(f\"Memory usage of the tensor: {memory_usage * 279//1024} KB\") # 279 is the number of samples we have in the dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:36.112121Z","iopub.execute_input":"2024-12-18T16:54:36.112465Z","iopub.status.idle":"2024-12-18T16:54:36.117451Z","shell.execute_reply.started":"2024-12-18T16:54:36.112434Z","shell.execute_reply":"2024-12-18T16:54:36.116609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Define the path to the directory containing the train images\ntrain_image_path = '/kaggle/input/goodsad/cigarette_box/cigarette_box/train'\n\n# Load the train dataset using the ImageFolder dataset class\n# ImageFolder is a PyTorch dataset class for loading images from a directory\n# It automatically loads images from subdirectories and applies transformations to them\n# In this case, 'transform' is a transformation applied to each image in the dataset\n# It preprocesses the images before they are used for training\ngood_dataset = ImageFolder(root=train_image_path, transform=transform)\n\n# Access a sample from the dataset\n# In this case, we're accessing the first sample\n# x contains the preprocessed image data\n# y contains the corresponding label (class index)\nx, y = good_dataset[0]\n\n# Print the shape of the preprocessed image data (x) and its corresponding label (y)\nprint(\"Image Shape:\", x.shape)\nprint(\"Label:\", y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:38.401437Z","iopub.execute_input":"2024-12-18T16:54:38.401775Z","iopub.status.idle":"2024-12-18T16:54:39.129941Z","shell.execute_reply.started":"2024-12-18T16:54:38.401745Z","shell.execute_reply":"2024-12-18T16:54:39.129009Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AUTOENCODER","metadata":{}},{"cell_type":"markdown","source":"## Train test split for the autoencoder","metadata":{}},{"cell_type":"code","source":"\n# Split the dataset into training and testing subsets\n# The `torch.utils.data.random_split` function randomly splits a dataset into non-overlapping subsets\n# The first argument `good_dataset` is the dataset to be split\n# The second argument `[0.8, 0.2]` specifies the sizes of the subsets. Here, 80% for training and 20% for testing.\ntrain_dataset, test_dataset = torch.utils.data.random_split(good_dataset, [0.8, 0.2])\n\n# Print the lengths of the original dataset, training subset, and testing subset\nprint(\"Total number of samples in the original dataset:\", len(good_dataset))\nprint(\"Number of samples in the training subset:\", len(train_dataset))\nprint(\"Number of samples in the testing subset:\", len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:42.036121Z","iopub.execute_input":"2024-12-18T16:54:42.036785Z","iopub.status.idle":"2024-12-18T16:54:42.050164Z","shell.execute_reply.started":"2024-12-18T16:54:42.036755Z","shell.execute_reply":"2024-12-18T16:54:42.049412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using dataloader for efficient data loading during training","metadata":{}},{"cell_type":"code","source":"\n# Set the batch size\nBS = 16\n\n# Create data loaders for training and testing datasets\ntrain_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BS, shuffle=True)\n\n# Get a batch of images and labels from the training loader\nimage_batch, label_batch = next(iter(train_loader))\n\n# Print the shape of the input images and labels\nprint(f'Shape of input images: {image_batch.shape}')\nprint(f'Shape of labels: {label_batch.shape}')\n# Calculate the memory usage\nmemory_usage = image_batch.numel() * image_batch.element_size()\n\n# Print the memory usage\nprint(f\"Memory usage of the tensor: {memory_usage//1024} KB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:45.963283Z","iopub.execute_input":"2024-12-18T16:54:45.964025Z","iopub.status.idle":"2024-12-18T16:54:48.831311Z","shell.execute_reply.started":"2024-12-18T16:54:45.963995Z","shell.execute_reply":"2024-12-18T16:54:48.830523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the figure size\nplt.figure(figsize=(12*4, 48*4))\n\n# Create a grid of images from the image batch and visualize it\ngrid = torchvision.utils.make_grid(image_batch[0:4], padding=5, nrow=4)\nplt.imshow(grid.permute(1, 2, 0))  # Permute dimensions to (height, width, channels) for visualization\nplt.title('Good Samples')  # Set the title of the plot\nplt.show()  # Show the plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:48.948153Z","iopub.execute_input":"2024-12-18T16:54:48.949004Z","iopub.status.idle":"2024-12-18T16:54:50.021097Z","shell.execute_reply.started":"2024-12-18T16:54:48.948967Z","shell.execute_reply":"2024-12-18T16:54:50.020288Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train autoencoder model","metadata":{}},{"cell_type":"code","source":"class Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 128, kernel_size=4),\n            nn.ReLU(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(128, 256, kernel_size=4),\n            nn.ReLU(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(256, 256, kernel_size=3),\n            nn.ReLU(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, output_padding=1 ),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 3, kernel_size=5, stride=2, output_padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n# Test the autoencoder architecture\nmodel = Autoencoder()  \ninput_image = torch.randn(1, 3, 224, 224)  # Sample input image\noutput_image = model(input_image)\nprint(output_image.shape)  # Print the shape of the output image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:53.078046Z","iopub.execute_input":"2024-12-18T16:54:53.078725Z","iopub.status.idle":"2024-12-18T16:54:53.413718Z","shell.execute_reply.started":"2024-12-18T16:54:53.078694Z","shell.execute_reply":"2024-12-18T16:54:53.412845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:54:56.201869Z","iopub.execute_input":"2024-12-18T16:54:56.202710Z","iopub.status.idle":"2024-12-18T16:54:56.227723Z","shell.execute_reply.started":"2024-12-18T16:54:56.202672Z","shell.execute_reply":"2024-12-18T16:54:56.226848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ckpoints = torch.load('/kaggle/working/AE_GoodsAD_cigarette_box.pth')\nmodel.load_state_dict(ckpoints)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:55:19.000520Z","iopub.execute_input":"2024-12-18T16:55:19.001342Z","iopub.status.idle":"2024-12-18T16:55:19.173660Z","shell.execute_reply.started":"2024-12-18T16:55:19.001306Z","shell.execute_reply":"2024-12-18T16:55:19.172854Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.to(device)# Move the model to the GPU\n# criterion = torch.nn.MSELoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:35:53.468462Z","iopub.execute_input":"2024-12-18T09:35:53.469257Z","iopub.status.idle":"2024-12-18T09:35:53.477496Z","shell.execute_reply.started":"2024-12-18T09:35:53.469225Z","shell.execute_reply":"2024-12-18T09:35:53.476591Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training loop","metadata":{}},{"cell_type":"code","source":"# # Define a list to store training loss and validation loss\n# Loss = []\n# Validation_Loss = []\n\n\n# num_epochs = 100\n# for epoch in tqdm(range(num_epochs)):\n#     model.train()  # Set model to training mode\n#     for img, _ in train_loader:\n#         img = img.to(device)\n        \n#         output = model(img)\n#         loss = criterion(output, img)\n\n#         optimizer.zero_grad() #clears the gradients of all optimized tensors.  This step is necessary because gradients are accumulated by default in PyTorch, and we want to compute fresh gradients for the current batch of data.\n#         loss.backward() # This line computes the gradients of the loss function with respect to the model parameters. These gradients are used to update the model parameters during optimization.\n#         optimizer.step() # This line updates the model parameters using the computed gradients. \n#     Loss.append(loss.item())\n       \n\n#     # Calculate validation loss\n#     model.eval()  # Set model to evaluation mode\n#     with torch.no_grad():\n#         val_loss_sum = 0.0\n#         num_batches = 0\n#         for img, _ in test_loader:\n#             img = img.to(device)\n#             output = model(img)\n#             val_loss = criterion(output, img)\n#             val_loss_sum += val_loss.item()\n#             num_batches += 1\n#         val_loss_avg = val_loss_sum / num_batches\n#         Validation_Loss.append(val_loss_avg)\n    \n#     if epoch % 5 == 0:\n#         print('Epoch [{}/{}], Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item(), val_loss_avg))\n\n# plt.plot(Loss, label='Training Loss')\n# plt.plot(Validation_Loss, label='Validation Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n# # plt.savefig('AE_food_package.png')\n# plt.show()\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:35:55.767445Z","iopub.execute_input":"2024-12-18T09:35:55.768224Z","iopub.status.idle":"2024-12-18T09:39:48.321919Z","shell.execute_reply.started":"2024-12-18T09:35:55.768163Z","shell.execute_reply":"2024-12-18T09:39:48.321089Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Save the model\n# torch.save(model.state_dict(), 'test.pth')\n# model.eval()\n\n# ckpoints = torch.load('test.pth')\n# model.load_state_dict(ckpoints)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:43:47.626308Z","iopub.execute_input":"2024-12-18T09:43:47.626642Z","iopub.status.idle":"2024-12-18T09:43:47.660995Z","shell.execute_reply.started":"2024-12-18T09:43:47.626616Z","shell.execute_reply":"2024-12-18T09:43:47.660211Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reconstruction of good images","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    for data, _ in train_loader:\n        data = data.cuda()\n        recon = model(data)\n        break\n\nrecon_error =  ((data-recon)**2).mean(axis=1)\nprint(recon_error.shape)\n\nplt.figure(dpi=250)\nfig, ax = plt.subplots(3, 3, figsize=(5*4, 4*4))\nfor i in range(3):\n    ax[0, i].imshow(data[i].cpu().numpy().transpose((1, 2, 0)))\n    ax[1, i].imshow(recon[i].cpu().numpy().transpose((1, 2, 0)))\n    ax[2, i].imshow(recon_error[i][0:-10,0:-10].cpu().numpy(), cmap='jet',vmax= torch.max(recon_error[i])) #[0:-10,0:-10]\n    ax[0, i].axis('OFF')\n    ax[1, i].axis('OFF')\n    ax[2, i].axis('OFF')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:57:36.965481Z","iopub.execute_input":"2024-12-18T16:57:36.965837Z","iopub.status.idle":"2024-12-18T16:57:40.517265Z","shell.execute_reply.started":"2024-12-18T16:57:36.965793Z","shell.execute_reply":"2024-12-18T16:57:40.516347Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reconstruction of bad images","metadata":{}},{"cell_type":"markdown","source":"## Obtain the fault detection HEATMAP using AE","metadata":{}},{"cell_type":"code","source":"# test_image_1 = transform(Image.open(r'/kaggle/input/goodsad/cigarette_box/cigarette_box/test/opened/000_005.jpg'))\n# test_image_2 = transform(Image.open(r'/kaggle/input/goodsad/cigarette_box/cigarette_box/test/opened/000_004.jpg'))\n# test_image_3 = transform(Image.open(r'/kaggle/input/goodsad/cigarette_box/cigarette_box/test/opened/000_007.jpg'))\n\n# data = torch.stack([test_image_1,test_image_2, test_image_3])\n\n# with torch.no_grad():\n#     data = data.cuda()\n#     recon = model(data)\n    \n# recon_error =  ((data-recon)**2).mean(axis=1)\n    \n# plt.figure(dpi=250)\n# fig, ax = plt.subplots(3, 3, figsize=(5*4, 4*4))\n# for i in range(3):\n#     ax[0, i].imshow(data[i].cpu().numpy().transpose((1, 2, 0)))\n#     ax[1, i].imshow(recon[i].cpu().numpy().transpose((1, 2, 0)))\n#     ax[2, i].imshow(recon_error[i][0:-10,0:-10].cpu().numpy(), cmap='jet',vmax= torch.max(recon_error[i]))\n#     ax[0, i].axis('OFF')\n#     ax[1, i].axis('OFF')\n#     ax[2, i].axis('OFF')\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T16:55:42.390605Z","iopub.execute_input":"2024-12-18T16:55:42.391001Z","iopub.status.idle":"2024-12-18T16:55:44.003477Z","shell.execute_reply.started":"2024-12-18T16:55:42.390963Z","shell.execute_reply":"2024-12-18T16:55:44.002308Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Finding threshold","metadata":{}},{"cell_type":"code","source":"# RECON_ERROR=[]\n# with torch.no_grad():\n#     for data, _ in train_loader:\n#         data = data.cuda()\n#         recon = model(data)\n#         data_recon_squared_mean =  ((data-recon)**2).mean(axis=(1))[:,0:-10,0:-10].mean(axis=(1,2))\n        \n#         RECON_ERROR.append(data_recon_squared_mean)\n        \n# RECON_ERROR = torch.cat(RECON_ERROR).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:44:30.484150Z","iopub.execute_input":"2024-12-18T09:44:30.484515Z","iopub.status.idle":"2024-12-18T09:44:50.278514Z","shell.execute_reply.started":"2024-12-18T09:44:30.484486Z","shell.execute_reply":"2024-12-18T09:44:50.277737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# best_threshold = np.mean(RECON_ERROR) + 3 * np.std(RECON_ERROR)\n\n# plt.hist(RECON_ERROR,bins=50)\n# plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r') \n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:44:50.280151Z","iopub.execute_input":"2024-12-18T09:44:50.280508Z","iopub.status.idle":"2024-12-18T09:44:50.536396Z","shell.execute_reply.started":"2024-12-18T09:44:50.280473Z","shell.execute_reply":"2024-12-18T09:44:50.535572Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class_labels = []\n# y_true = []\n# y_pred = []\n# y_score = []\n\n\n# test_path = Path(r'/kaggle/input/goodsad/cigarette_box/cigarette_box/test')\n# class_dirs = [d.name for d in test_path.iterdir() if d.is_dir()]\n\n# model.eval()\n\n# for class_name in class_dirs:\n#     folder_path = test_path / class_name\n\n#     for pth in tqdm(folder_path.iterdir(),leave=False):\n\n#         class_label = pth.parts[-2]\n#         with torch.no_grad():\n#             test_image = transform(Image.open(pth)).cuda().unsqueeze(0)\n#             # test_image = test_image.repeat(1, 3, 1, 1)\n            \n#             recon_image = model(test_image)\n    \n            \n#             # y_score_image = \n#             y_score_image =  ((test_image - recon_image)**2).mean(axis=(1))[:,0:-10,0:-10].mean()\n            \n#             y_score.append(y_score_image.cpu())\n#             y_true.append(0 if class_label == 'good' else 1)\n            \n\n#         class_labels.append(class_label)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:44:50.537543Z","iopub.execute_input":"2024-12-18T09:44:50.537881Z","iopub.status.idle":"2024-12-18T09:45:34.842988Z","shell.execute_reply.started":"2024-12-18T09:44:50.537844Z","shell.execute_reply":"2024-12-18T09:45:34.842117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.hist(y_score,bins=50)\n# plt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:45:34.844896Z","iopub.execute_input":"2024-12-18T09:45:34.845539Z","iopub.status.idle":"2024-12-18T09:45:35.103580Z","shell.execute_reply.started":"2024-12-18T09:45:34.845501Z","shell.execute_reply":"2024-12-18T09:45:35.102743Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Calculate AUC-ROC score\n# auc_roc_score = roc_auc_score(y_true, y_score)\n# print(\"AUC-ROC Score:\", auc_roc_score)\n\n# # Plot ROC curve\n# fpr, tpr, thresholds = roc_curve(y_true, y_score)\n# plt.figure()\n# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)\n# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.title('Receiver Operating Characteristic (ROC) Curve')\n# plt.legend(loc=\"lower right\")\n# plt.savefig('Result_AE_food_package.png')\n# plt.show()\n\n\nf1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]\n# Select the best threshold based on F1 score\nbest_threshold = thresholds[np.argmax(f1_scores)]\n\nprint(f'best_threshold = {best_threshold}')\n\naccuracy = accuracy_score(y_true, (y_score >= best_threshold).astype(int))\nprint(\"Accuracy:\", accuracy)\n\nf1 = f1_score(y_true, (y_score >= best_threshold).astype(int))\nprint(\"F1 Score:\", f1)\n\n# # Generate confusion matrix\n# cm = confusion_matrix(y_true, y_pred)\n# disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['OK','NOK'])\n# disp.plot()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:37:47.873863Z","iopub.execute_input":"2024-12-18T08:37:47.874583Z","iopub.status.idle":"2024-12-18T08:37:47.884275Z","shell.execute_reply.started":"2024-12-18T08:37:47.874546Z","shell.execute_reply":"2024-12-18T08:37:47.883586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# import numpy as np\n\n\n# y_predict = (y_score >= best_threshold).astype(int)\n# # Convert list_1 to binary labels\n# binary_labels = [0 if label == 'good' else 1 for label in class_labels]\n\n# # Calculate accuracy for each label\n# unique_labels = np.unique(class_labels)\n# accuracies = []\n# for label in unique_labels:\n#     label_mask = np.array(class_labels) == label\n#     label_accuracy = accuracy_score(np.array(binary_labels)[label_mask], np.array(y_predict)[label_mask])\n#     accuracies.append(label_accuracy)\n\n# # Plot accuracy for each label\n# plt.figure()\n# plt.bar(unique_labels, accuracies, color='skyblue')\n# plt.xlabel('Labels')\n# plt.ylabel('Accuracy')\n# plt.title('Accuracy for Each Label')\n# plt.ylim(0, 1)\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:45:40.221681Z","iopub.execute_input":"2024-12-18T09:45:40.222351Z","iopub.status.idle":"2024-12-18T09:45:40.414769Z","shell.execute_reply.started":"2024-12-18T09:45:40.222319Z","shell.execute_reply":"2024-12-18T09:45:40.413923Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# KNN+RESNET","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:52:16.178746Z","iopub.execute_input":"2024-12-19T09:52:16.179107Z","iopub.status.idle":"2024-12-19T09:52:16.214841Z","shell.execute_reply.started":"2024-12-19T09:52:16.179049Z","shell.execute_reply":"2024-12-19T09:52:16.213942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n\nmodel = torch.nn.Sequential(*list(resnet_model.children())[:-1]).to(device)\nmodel.eval()\n\nfor param in model.parameters():\n    param.requires_grad = False\n\ndel resnet_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:52:17.587481Z","iopub.execute_input":"2024-12-19T09:52:17.588152Z","iopub.status.idle":"2024-12-19T09:52:18.963704Z","shell.execute_reply.started":"2024-12-19T09:52:17.588114Z","shell.execute_reply":"2024-12-19T09:52:18.962884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = ResNet50_Weights.DEFAULT.transforms()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:52:19.417946Z","iopub.execute_input":"2024-12-19T09:52:19.418909Z","iopub.status.idle":"2024-12-19T09:52:19.423047Z","shell.execute_reply.started":"2024-12-19T09:52:19.418873Z","shell.execute_reply":"2024-12-19T09:52:19.421948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Memory Bank ","metadata":{}},{"cell_type":"code","source":"memory_bank =[]\n\nfolder_path = Path(r'/kaggle/input/goodsad/drink_bottle/drink_bottle/train/good')\n\nfor pth in tqdm(folder_path.iterdir(),leave=False):\n\n    with torch.no_grad():\n        data = transform(Image.open(pth)).to(device).unsqueeze(0)\n        features = model(data)\n        memory_bank.append(features.squeeze().cpu().detach())\n\nmemory_bank = torch.stack(memory_bank).cuda()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"values,indices = torch.sort(memory_bank.std(dim=0))\n\nplt.plot(values.cpu().numpy()[::-1])\nplt.vlines(x=500,ymin=0,ymax=0.5,colors='red')\nplt.ylim([0,0.5])\nplt.ylabel(\"std of the features\")\nplt.xlabel(\"Number of features\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:53:27.485315Z","iopub.status.idle":"2024-12-19T09:53:27.485585Z","shell.execute_reply.started":"2024-12-19T09:53:27.485452Z","shell.execute_reply":"2024-12-19T09:53:27.485466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"values,indices = torch.sort(memory_bank.std(dim=0))\nselected_indices = indices[-500:]\nmemory_bank = memory_bank[:,selected_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:53:27.486381Z","iopub.status.idle":"2024-12-19T09:53:27.486645Z","shell.execute_reply.started":"2024-12-19T09:53:27.486515Z","shell.execute_reply":"2024-12-19T09:53:27.486529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"memory_bank.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:53:27.487855Z","iopub.status.idle":"2024-12-19T09:53:27.488204Z","shell.execute_reply.started":"2024-12-19T09:53:27.488021Z","shell.execute_reply":"2024-12-19T09:53:27.488036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Finding Threshold to classify","metadata":{}},{"cell_type":"code","source":"dist_error=[]\n\nk=50\n\nfolder_path = Path(r'/kaggle/input/goodsad/drink_bottle/drink_bottle/train/good')\n\nfor pth in tqdm(folder_path.iterdir(),leave=False):\n    data = transform(Image.open(pth)).cuda().unsqueeze(0)\n    with torch.no_grad():\n        features = model(data).squeeze()\n    dist,_=torch.sort(torch.norm(memory_bank - features[selected_indices], dim=1))# Calculating the pair-wise distance between the sample and memory bank\n    dist = dist[:k].mean()# K nearsest neighbours\n    dist_error.append(dist.cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:53:27.489552Z","iopub.status.idle":"2024-12-19T09:53:27.489845Z","shell.execute_reply.started":"2024-12-19T09:53:27.489705Z","shell.execute_reply":"2024-12-19T09:53:27.489721Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_threshold = np.mean(dist_error) + 1.5 * np.std(dist_error)\n\nplt.hist(y_score,bins=50)\nplt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:53:27.490747Z","iopub.status.idle":"2024-12-19T09:53:27.491009Z","shell.execute_reply.started":"2024-12-19T09:53:27.490876Z","shell.execute_reply":"2024-12-19T09:53:27.490889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classify the test dataset","metadata":{}},{"cell_type":"code","source":"class_labels = []\ny_true = []\nresnet_features = []\ny_score = []\n\nparent_dir = Path(r'/kaggle/input/goodsad/drink_bottle/drink_bottle/test')\nclass_dirs = [d.name for d in parent_dir.iterdir() if d.is_dir()]\n\nfor class_name in class_dirs:\n    folder_path = parent_dir / class_name\n\n    for pth in tqdm(folder_path.iterdir(),leave=False):\n\n        class_label = pth.parts[-2]\n        with torch.no_grad():\n            test_image = transform(Image.open(pth)).cuda().unsqueeze(0)\n            features = model(test_image).squeeze()    \n            dist,_=torch.sort(torch.norm(memory_bank - features[selected_indices], dim=1))#[-10:].mean()\n            dist = dist[:k].mean()\n            y_score.append(dist.cpu().numpy())\n\n        class_labels.append(class_label)\n        y_true.append(0 if class_label == 'good' else 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:53:27.492000Z","iopub.status.idle":"2024-12-19T09:53:27.492325Z","shell.execute_reply.started":"2024-12-19T09:53:27.492178Z","shell.execute_reply":"2024-12-19T09:53:27.492194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_score_nok = [score  for score,true in zip(y_score,y_true) if true==1]\nplt.hist(y_score_nok,bins=50)\nplt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:53:27.493449Z","iopub.status.idle":"2024-12-19T09:53:27.493745Z","shell.execute_reply.started":"2024-12-19T09:53:27.493596Z","shell.execute_reply":"2024-12-19T09:53:27.493612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Calculate AUC-ROC score\nauc_roc_score = roc_auc_score(y_true, y_score)\nprint(\"AUC-ROC Score:\", auc_roc_score)\n\n# Plot ROC curve\nfpr, tpr, thresholds = roc_curve(y_true, y_score)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.savefig('Result_KNN-ResNet_drink_bottle.png')\nplt.show()\n\nf1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]\n# Select the best threshold based on F1 score\nbest_threshold = thresholds[np.argmax(f1_scores)]\n\nprint(f'best_threshold = {best_threshold}')\n\naccuracy = accuracy_score(y_true, (y_score >= best_threshold).astype(int))\nprint(\"Accuracy:\", accuracy)\n\nf1 = f1_score(y_true, (y_score >= best_threshold).astype(int))\nprint(\"F1 Score:\", f1)\n\n# Generate confusion matrix\ncm = confusion_matrix(y_true, (y_score >= best_threshold).astype(int))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['OK','NOK'])\ndisp.plot()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:53:27.495169Z","iopub.status.idle":"2024-12-19T09:53:27.495432Z","shell.execute_reply.started":"2024-12-19T09:53:27.495302Z","shell.execute_reply":"2024-12-19T09:53:27.495315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport numpy as np\n\n\ny_predict = (y_score >= best_threshold).astype(int)\n# Convert list_1 to binary labels\nbinary_labels = [0 if label == 'good' else 1 for label in class_labels]\n\n# Calculate accuracy for each label\nunique_labels = np.unique(class_labels)\naccuracies = []\nfor label in unique_labels:\n    label_mask = np.array(class_labels) == label\n    label_accuracy = accuracy_score(np.array(binary_labels)[label_mask], np.array(y_predict)[label_mask])\n    accuracies.append(label_accuracy)\n\n# Plot accuracy for each label\nplt.figure()\nplt.bar(unique_labels, accuracies, color='skyblue')\nplt.xlabel('Labels')\nplt.ylabel('Accuracy')\nplt.title('Accuracy for Each Label')\nplt.ylim(0, 1)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T09:03:39.456586Z","iopub.execute_input":"2024-12-18T09:03:39.456848Z","iopub.status.idle":"2024-12-18T09:03:39.645018Z","shell.execute_reply.started":"2024-12-18T09:03:39.456822Z","shell.execute_reply":"2024-12-18T09:03:39.644223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Resnet + AutoEncoder","metadata":{}},{"cell_type":"markdown","source":"### Setup + Visualize","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:33:13.280572Z","iopub.execute_input":"2024-12-19T18:33:13.281366Z","iopub.status.idle":"2024-12-19T18:33:13.285550Z","shell.execute_reply.started":"2024-12-19T18:33:13.281332Z","shell.execute_reply":"2024-12-19T18:33:13.284797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FeatCAE(nn.Module):\n    \"\"\"Autoencoder.\"\"\"\n\n    def __init__(self, in_channels=1000, latent_dim=50, is_bn=True):\n        super(FeatCAE, self).__init__()\n\n        layers = []\n        layers += [nn.Conv2d(in_channels, (in_channels + 2 * latent_dim) // 2, kernel_size=1, stride=1, padding=0)]\n        if is_bn:\n            layers += [nn.BatchNorm2d(num_features=(in_channels + 2 * latent_dim) // 2)]\n        layers += [nn.ReLU()]\n        layers += [nn.Conv2d((in_channels + 2 * latent_dim) // 2, 2 * latent_dim, kernel_size=1, stride=1, padding=0)]\n        if is_bn:\n            layers += [nn.BatchNorm2d(num_features=2 * latent_dim)]\n        layers += [nn.ReLU()]\n        layers += [nn.Conv2d(2 * latent_dim, latent_dim, kernel_size=1, stride=1, padding=0)]\n\n        self.encoder = nn.Sequential(*layers)\n\n        # if 1x1 conv to reconstruct the rgb values, we try to learn a linear combination\n        # of the features for rgb\n        layers = []\n        layers += [nn.Conv2d(latent_dim, 2 * latent_dim, kernel_size=1, stride=1, padding=0)]\n        if is_bn:\n            layers += [nn.BatchNorm2d(num_features=2 * latent_dim)]\n        layers += [nn.ReLU()]\n        layers += [nn.Conv2d(2 * latent_dim, (in_channels + 2 * latent_dim) // 2, kernel_size=1, stride=1, padding=0)]\n        if is_bn:\n            layers += [nn.BatchNorm2d(num_features=(in_channels + 2 * latent_dim) // 2)]\n        layers += [nn.ReLU()]\n        layers += [nn.Conv2d((in_channels + 2 * latent_dim) // 2, in_channels, kernel_size=1, stride=1, padding=0)]\n        # layers += [nn.ReLU()]\n\n        self.decoder = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:31:45.381553Z","iopub.execute_input":"2024-12-19T18:31:45.381929Z","iopub.status.idle":"2024-12-19T18:31:45.390892Z","shell.execute_reply.started":"2024-12-19T18:31:45.381898Z","shell.execute_reply":"2024-12-19T18:31:45.390253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class resnet_feature_extractor(torch.nn.Module):\n    def __init__(self):\n        \"\"\"This class extracts the feature maps from a pretrained Resnet model.\"\"\"\n        super(resnet_feature_extractor, self).__init__()\n        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n\n        self.model.eval()\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        \n\n        # Hook to extract feature maps\n        def hook(module, input, output) -> None:\n            \"\"\"This hook saves the extracted feature map on self.featured.\"\"\"\n            self.features.append(output)\n\n        self.model.layer2[-1].register_forward_hook(hook)            \n        self.model.layer3[-1].register_forward_hook(hook) \n\n    def forward(self, input):\n\n        self.features = []\n        with torch.no_grad():\n            _ = self.model(input)\n\n        self.avg = torch.nn.AvgPool2d(3, stride=1)\n        fmap_size = self.features[0].shape[-2]         # Feature map sizes h, w\n        self.resize = torch.nn.AdaptiveAvgPool2d(fmap_size)\n\n        resized_maps = [self.resize(self.avg(fmap)) for fmap in self.features]\n        patch = torch.cat(resized_maps, 1)            # Merge the resized feature maps\n\n        return patch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:31:47.018562Z","iopub.execute_input":"2024-12-19T18:31:47.018902Z","iopub.status.idle":"2024-12-19T18:31:47.026190Z","shell.execute_reply.started":"2024-12-19T18:31:47.018875Z","shell.execute_reply":"2024-12-19T18:31:47.025453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor()\n])\n\ntrain_image_path = Path('/kaggle/input/goodsad/food_package/food_package/train')\n\ngood_dataset = ImageFolder(root=train_image_path, transform=transform)\ntrain_dataset, test_dataset = torch.utils.data.random_split(good_dataset, [0.8, 0.2])\n\n# Set the batch size\nBS = 16\n\n# Create data loaders for training and testing datasets\ntrain_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BS, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:31:49.096101Z","iopub.execute_input":"2024-12-19T18:31:49.096915Z","iopub.status.idle":"2024-12-19T18:31:49.107432Z","shell.execute_reply.started":"2024-12-19T18:31:49.096881Z","shell.execute_reply":"2024-12-19T18:31:49.106719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = Image.open(r'/kaggle/input/goodsad/cigarette_box/cigarette_box/test/good/000_001.jpg')\nimage = transform(image).unsqueeze(0)\n\nbackbone = resnet_feature_extractor()\nfeature = backbone(image)\n\nprint(backbone.features[0].shape)\nprint(backbone.features[1].shape)\n\nprint(feature.shape)\n\nplt.imshow(image[0].permute(1,2,0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:31:51.050634Z","iopub.execute_input":"2024-12-19T18:31:51.051358Z","iopub.status.idle":"2024-12-19T18:31:52.659134Z","shell.execute_reply.started":"2024-12-19T18:31:51.051324Z","shell.execute_reply":"2024-12-19T18:31:52.658444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select 10 random indices for feature maps\nindices = torch.randperm(1536)[:10]\n\n# Plot the selected feature maps\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfor i, idx in enumerate(indices):\n    row = i // 5\n    col = i % 5\n    axes[row, col].imshow(feature[0,idx].detach().cpu(), cmap='gray')\n    axes[row, col].set_title(f'Feature Map {idx}')\n    axes[row, col].axis('off')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:31:58.428265Z","iopub.execute_input":"2024-12-19T18:31:58.428806Z","iopub.status.idle":"2024-12-19T18:31:58.947441Z","shell.execute_reply.started":"2024-12-19T18:31:58.428773Z","shell.execute_reply":"2024-12-19T18:31:58.946765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"model = FeatCAE(in_channels=1536, latent_dim=100).to(device)\nbackbone.to(device)\n# Define loss function and optimizer\ncriterion = torch.nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:33:19.882013Z","iopub.execute_input":"2024-12-19T18:33:19.882609Z","iopub.status.idle":"2024-12-19T18:33:19.910142Z","shell.execute_reply.started":"2024-12-19T18:33:19.882576Z","shell.execute_reply":"2024-12-19T18:33:19.909378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a list to store training loss and validation loss\nLoss = []\nValidation_Loss = []\n\n\nnum_epochs = 70\nfor epoch in tqdm(range(num_epochs)):\n    model.train()\n    for data,_ in train_loader:\n        with torch.no_grad():\n            features = backbone(data.to(device))\n        # Forward pass\n        output = model(features)\n        # Compute the loss\n        loss = criterion(output, features)\n        # Backpropagation and optimization step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    Loss.append(loss.item())\n\n    # Calculate validation loss\n    model.eval()  # Set model to evaluation mode\n    with torch.no_grad():\n        val_loss_sum = 0.0\n        num_batches = 0\n        for data, _ in test_loader:\n            features = backbone(data.to(device))\n            output = model(features)\n            val_loss = criterion(output, features)\n            val_loss_sum += val_loss.item()\n            num_batches += 1\n        val_loss_avg = val_loss_sum / num_batches\n        Validation_Loss.append(val_loss_avg)\n    \n    if epoch % 5 == 0:\n        print('Epoch [{}/{}], Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch + 1, num_epochs, loss.item(), val_loss_avg))\n\nplt.plot(Loss, label='Training Loss')\nplt.plot(Validation_Loss, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.savefig('AE-Resnet_food_package.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T16:52:14.917282Z","iopub.execute_input":"2024-12-19T16:52:14.918130Z","iopub.status.idle":"2024-12-19T17:57:48.155720Z","shell.execute_reply.started":"2024-12-19T16:52:14.918055Z","shell.execute_reply":"2024-12-19T17:57:48.154934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), 'AE-Resnet_GoodsAD_food_package.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:33:25.707298Z","iopub.execute_input":"2024-12-19T18:33:25.707633Z","iopub.status.idle":"2024-12-19T18:33:25.746956Z","shell.execute_reply.started":"2024-12-19T18:33:25.707605Z","shell.execute_reply":"2024-12-19T18:33:25.746216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ckpoints = torch.load('/kaggle/working/AE-Resnet_GoodsAD_drink_bottle.pth')\nmodel.load_state_dict(ckpoints)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T07:06:18.866790Z","iopub.execute_input":"2024-12-19T07:06:18.867048Z","iopub.status.idle":"2024-12-19T07:06:18.886168Z","shell.execute_reply.started":"2024-12-19T07:06:18.867024Z","shell.execute_reply":"2024-12-19T07:06:18.885306Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = Image.open(r'/kaggle/input/goodsad/cigarette_box/cigarette_box/test/good/000_001.jpg')\nimage = transform(image).unsqueeze(0)\n\nwith torch.no_grad():\n    features = backbone(image.cuda())\n    recon = model(features)\n\nrecon_error =  ((features-recon)**2).mean(axis=(1)).unsqueeze(0)\n\nsegm_map = torch.nn.functional.interpolate(     # Upscale by bi-linaer interpolation to match the original input resolution\n                recon_error,\n                size=(224, 224),\n                mode='bilinear'\n            )\n\nplt.imshow(segm_map.squeeze().cpu().numpy(), cmap='jet')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:33:31.105337Z","iopub.execute_input":"2024-12-19T18:33:31.105696Z","iopub.status.idle":"2024-12-19T18:33:31.325919Z","shell.execute_reply.started":"2024-12-19T18:33:31.105668Z","shell.execute_reply":"2024-12-19T18:33:31.325048Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finding Threshold","metadata":{}},{"cell_type":"code","source":"def decision_function(segm_map):  \n\n    mean_top_10_values = []\n\n    for map in segm_map:\n        # Flatten the tensor\n        flattened_tensor = map.reshape(-1)\n\n        # Sort the flattened tensor along the feature dimension (descending order)\n        sorted_tensor, _ = torch.sort(flattened_tensor,descending=True)\n\n        # Take the top 10 values along the feature dimension\n        mean_top_10_value = sorted_tensor[:10].mean()\n\n        mean_top_10_values.append(mean_top_10_value)\n\n    return torch.stack(mean_top_10_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:33:38.395902Z","iopub.execute_input":"2024-12-19T18:33:38.396675Z","iopub.status.idle":"2024-12-19T18:33:38.400871Z","shell.execute_reply.started":"2024-12-19T18:33:38.396641Z","shell.execute_reply":"2024-12-19T18:33:38.400203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\nANOMALY_ERROR=[]\nfor data,_ in train_loader:\n    \n    with torch.no_grad():\n        features = backbone(data.cuda()).squeeze()\n        # Forward pass\n        recon = model(features)\n    # Compute the loss\n    segm_map =  ((features-recon)**2).mean(axis=(1))[:,3:-3,3:-3]\n    anomaly_score = decision_function(segm_map)\n    # anomaly_score = segm_map.mean(axis=(1,2))\n    \n    ANOMALY_ERROR.append(anomaly_score)\n    \nANOMALY_ERROR = torch.cat(ANOMALY_ERROR).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:33:42.201805Z","iopub.execute_input":"2024-12-19T18:33:42.202326Z","iopub.status.idle":"2024-12-19T18:33:44.762636Z","shell.execute_reply.started":"2024-12-19T18:33:42.202288Z","shell.execute_reply":"2024-12-19T18:33:44.761771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_threshold = np.mean(ANOMALY_ERROR) + 3 * np.std(ANOMALY_ERROR)\n\nheat_map_max, heat_map_min = np.max(ANOMALY_ERROR), np.min(ANOMALY_ERROR)\n\nplt.hist(ANOMALY_ERROR,bins=50)\nplt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T17:58:34.597339Z","iopub.execute_input":"2024-12-19T17:58:34.597694Z","iopub.status.idle":"2024-12-19T17:58:34.861978Z","shell.execute_reply.started":"2024-12-19T17:58:34.597649Z","shell.execute_reply":"2024-12-19T17:58:34.861106Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Classify the Test","metadata":{}},{"cell_type":"code","source":"class_labels = []\ny_true=[]\ny_pred=[]\ny_score=[]\n\nmodel.eval()\nbackbone.eval()\n\ntest_path = Path(r'/kaggle/input/goodsad/food_package/food_package/test')\nclass_dirs = [d.name for d in test_path.iterdir() if d.is_dir()]\n\nfor class_name in class_dirs:\n    folder_path = test_path / class_name\n\n    for pth in tqdm(folder_path.iterdir(),leave=False):\n\n        class_label = pth.parts[-2]\n        with torch.no_grad():\n            test_image = transform(Image.open(pth)).cuda().unsqueeze(0)\n            features = backbone(test_image)\n            # Forward pass\n            recon = model(features)\n            segm_map = ((features - recon)**2).mean(axis=(1))[:,3:-3,3:-3]\n            y_score_image = decision_function(segm_map=segm_map)\n            # y_score_image = segm_map.mean(axis=(1,2))\n\n            y_score.append(y_score_image.cpu().numpy())\n            y_true.append(0 if class_label == 'good' else 1)\n            \n        class_labels.append(class_label)\n            \ny_true = np.array(y_true)\ny_score = np.array(y_score)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T17:58:34.863058Z","iopub.execute_input":"2024-12-19T17:58:34.863405Z","execution_failed":"2024-12-19T17:58:53.987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.hist(y_score,bins=50)\nplt.vlines(x=best_threshold,ymin=0,ymax=30,color='r')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-19T17:58:53.987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate AUC-ROC score\nauc_roc_score = roc_auc_score(y_true, y_score)\nprint(\"AUC-ROC Score:\", auc_roc_score)\n\n# Plot ROC curve\nfpr, tpr, thresholds = roc_curve(y_true, y_score)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc_roc_score)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.savefig('Result_AE-Resnet_food_package.png')\nplt.show()\n\n\nf1_scores = [f1_score(y_true, y_score >= threshold) for threshold in thresholds]\n# Select the best threshold based on F1 score\nbest_threshold = thresholds[np.argmax(f1_scores)]\n\nprint(f'best_threshold = {best_threshold}')\n\naccuracy = accuracy_score(y_true, (y_score >= best_threshold).astype(int))\nprint(\"Accuracy:\", accuracy)\n\nf1 = f1_score(y_true, (y_score >= best_threshold).astype(int))\nprint(\"F1 Score:\", f1)\n\n# Generate confusion matrix\ncm = confusion_matrix(y_true, (y_score >= best_threshold).astype(int))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['OK','NOK'])\ndisp.plot()\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-19T17:58:53.987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimport numpy as np\n\n\ny_predict = (y_score >= best_threshold).astype(int)\n# Convert list_1 to binary labels\nbinary_labels = [0 if label == 'good' else 1 for label in class_labels]\n\n# Calculate accuracy for each label\nunique_labels = np.unique(class_labels)\naccuracies = []\nfor label in unique_labels:\n    label_mask = np.array(class_labels) == label\n    label_accuracy = accuracy_score(np.array(binary_labels)[label_mask], np.array(y_predict)[label_mask])\n    accuracies.append(label_accuracy)\n\n# Plot accuracy for each label\nplt.figure()\nplt.bar(unique_labels, accuracies, color='skyblue')\nplt.xlabel('Labels')\nplt.ylabel('Accuracy')\nplt.title('Accuracy for Each Label')\nplt.ylim(0, 1)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2024-12-19T17:58:53.987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize Prediction","metadata":{}},{"cell_type":"code","source":"# import cv2, time\n# from IPython.display import clear_output\n\n# model.eval()\n# backbone.eval()\n\n# test_path = Path('/kaggle/input/goodsad/food_package/food_package/test')\n\n# for path in test_path.glob('*/*.png'):\n#     fault_type = path.parts[-2]\n#     if fault_type == 'good':\n#         continue\n#     test_image = transform(Image.open(path)).cuda().unsqueeze(0)\n#     true_path = str(path).replace('/test/', '/ground_truth/').replace('.png', '_mask.png')\n#     true_image = transform(Image.open(true_path)).cuda().unsqueeze(0)\n#     with torch.no_grad():\n#         features = backbone(test_image)\n#         # Forward pass\n#         recon = model(features)\n    \n#     segm_map = ((features - recon)**2).mean(axis=(1))\n#     y_score_image = decision_function(segm_map=segm_map)\n#     # y_score_image = segm_map.mean(axis=(1,2))\n    \n#     y_pred_image = 1*(y_score_image >= best_threshold)\n#     class_label = ['OK','NOK']\n\n#     plt.figure(figsize=(20,5))\n\n#     plt.subplot(1,4,1)\n#     plt.imshow(test_image.squeeze().permute(1,2,0).cpu().numpy())\n#     plt.title(f'fault type: {fault_type}')\n\n#     plt.subplot(1,4,2)\n#     heat_map = segm_map.squeeze().cpu().numpy()\n#     heat_map = heat_map\n#     heat_map = cv2.resize(heat_map, (128,128))\n#     plt.imshow(heat_map, cmap='jet', vmin=heat_map_min, vmax=heat_map_max*10) # Here I am cheating by multiplying by 10 (obtained using trail error)\n#     plt.title(f'Anomaly score: {y_score_image[0].cpu().numpy() / best_threshold:0.4f} || {class_label[y_pred_image]}')\n\n#     plt.subplot(1,4,3)\n#     plt.imshow((heat_map > best_threshold * 10), cmap='gray')\n#     plt.title(f'segmentation map')\n\n#     plt.subplot(1,4,4)\n#     plt.imshow(true_image.squeeze().cpu().numpy())\n#     plt.title(f'groundtruth')\n    \n#     plt.show()\n\n\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T08:39:42.175435Z","iopub.execute_input":"2024-12-18T08:39:42.175754Z","iopub.status.idle":"2024-12-18T08:39:42.185452Z","shell.execute_reply.started":"2024-12-18T08:39:42.175715Z","shell.execute_reply":"2024-12-18T08:39:42.184645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Specify the path to the file you want to remove\nfile_path = \"/kaggle/working/AE-Resnet_cigarette_box.png\"\n\n# Check if the file exists, then remove it\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(f\"File '{file_path}' has been removed.\")\nelse:\n    print(f\"File '{file_path}' does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T18:49:53.066934Z","iopub.execute_input":"2024-12-19T18:49:53.067785Z","iopub.status.idle":"2024-12-19T18:49:53.071997Z","shell.execute_reply.started":"2024-12-19T18:49:53.067752Z","shell.execute_reply":"2024-12-19T18:49:53.071342Z"}},"outputs":[],"execution_count":null}]}